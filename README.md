# Ensemble Learning Lab: Combining Models for Improved Performance

## Overview
This lab is based on Chapter 7 of *Python Machine Learning (Second Edition)* by Sebastian Raschka and Vahid Mirjalili. The chapter covers ensemble learning methods that combine multiple models to achieve better predictive performance than individual models.

In this hands-on lab, you will:

- Implement majority voting classifiers from scratch
- Use bagging with decision trees
- Apply AdaBoost for boosting weak learners
- Compare ensemble methods with individual classifiers
- Evaluate and tune ensemble classifiers

The lab uses the **Wine dataset** and **Iris dataset** for classification tasks.

---

## Objectives
- Understand the concept of ensemble learning and why it often outperforms individual models
- Implement and evaluate majority voting classifiers
- Apply bagging with bootstrap sampling
- Understand and implement AdaBoost for sequential learning
- Compare the performance of different ensemble methods
- Learn hyperparameter tuning for ensemble classifiers

---

## Prerequisites
- Python 3.x
- Libraries: `numpy`, `pandas`, `matplotlib`, `scikit-learn`

Install via pip:

```bash
pip install numpy pandas matplotlib scikit-learn
